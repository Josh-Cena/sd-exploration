{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S&DS 431 final project: code demo\n",
    "\n",
    "- Title: A study on Distribution Matching Distillation Models\n",
    "- Authors: Sida Chen, Jack Chen\n",
    "\n",
    "This Jupyter notebook is only intended for demonstration. You can run our project without using this notebook, but by running each Python script via command line. This file solely functions as a driver that connects all these scripts, explains their inputs and outputs, and runs them in the right order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "The exact Python packages' versions used to run this project can be found in `requirements.txt`. Apart from the standard libraries (like `diffusers`, `transformers`, `torch`, etc.), the most notable ones we need are `clip` and `pytorch-fid`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "We start by preparing the real images and generated images. The real images (used for FID analysis) is a subset of the ImageNet-64 dataset. You can download the zip and unzip it with:\n",
    "\n",
    "```bash\n",
    "wget -c https://image-net.org/data/downsample/Imagenet64_train_part1_npz.zip\n",
    "unzip Imagenet64_train_part1_npz.zip\n",
    "```\n",
    "\n",
    "The dataset consists of binary NumPy dumps, so we need to convert them to real PNG images first. This is implemented in `process_imagenet.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from process_imagenet import extract_images_from_npz\n",
    "\n",
    "npz_folder = \"Imagenet64_train_part1_npz/\"\n",
    "output_folder = \"real_images/\"\n",
    "extract_images_from_npz(npz_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated images are generated by the pre-trained `tianweiy/DMD2` model. The core logic is implemented by `schedule_variation.py`. If you run this file in command line, you need to pass the prompt, number of inference steps, and the scheduler, such as:\n",
    "\n",
    "```bash\n",
    "python schedule_variation.py --prompt \"a cat\" --scheduler \"linear\" --num-inference-steps 20\n",
    "```\n",
    "\n",
    "Or you can invoke the function like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from schedule_variation import generate_image\n",
    "\n",
    "# A list of jobs, each job is a tuple of (prompt, schedule, steps), and the seed\n",
    "generate_image([(\"a cat\", \"linear\", 20)], 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command line does not support setting a fixed random seed, so the output will be at `output/None`. In our investigations, we fix the random seed, causing each job to output to `output/{seed}`. To acquire all outputs we use, run `all_jobs.py`. Here are all jobs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from all_jobs import prompts, jobs, seeds\n",
    "\n",
    "print(\"Prompts:\")\n",
    "print(\"\\n\".join(prompts))\n",
    "print(\"Jobs:\")\n",
    "print(\"\\n\".join([f\"{prompt} {schedule} {steps}\" for prompt, schedule, steps in jobs]))\n",
    "print(\"Seeds:\")\n",
    "print(\", \".join([str(seed) for seed in seeds]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send them off with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in seeds:\n",
    "    generate_image(jobs, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This produces a folder tree in the following format:\n",
    "\n",
    "```plain\n",
    "output\n",
    "├── 223\n",
    "│   ├── A_photo_of_llama_cosine_15.png\n",
    "│   ├── A_photo_of_llama_cosine_3.png\n",
    "│   ├── A_photo_of_llama_cosine_4.png\n",
    "│   ├── A_photo_of_llama_cosine_8.png\n",
    "│   ├── A_photo_of_llama_exponential_15.png\n",
    "│   ├── A_photo_of_llama_exponential_3.png\n",
    "│   ├── A_photo_of_llama_exponential_4.png\n",
    "│   ├── A_photo_of_llama_exponential_8.png\n",
    "│   ├── A_photo_of_llama_linear_15.png\n",
    "│   ├── A_photo_of_llama_linear_3.png\n",
    "│   ├── A_photo_of_llama_linear_4.png\n",
    "│   ├── A_photo_of_llama_linear_8.png\n",
    "│   ├── a_shiba_inu_wearing_cosine_15.png\n",
    "│   ├── ...\n",
    "│   └── times.csv\n",
    "├── ...\n",
    "└── 69\n",
    "    ├── ...\n",
    "    └── times.csv\n",
    "```\n",
    "\n",
    "To facilitate group-based analysis, we will regroup them in another folder, such that the images are grouped by scheduler/steps and then identified by prompt/seed. This is implemented in `reorder_images.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reorder_images import reorder_images\n",
    "\n",
    "reorder_images(\"output\", \"output-alt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating scores\n",
    "\n",
    "We will calculate the following metrics:\n",
    "\n",
    "- Inference time - for evaluating runtime performance\n",
    "- [CLIP score](https://arxiv.org/pdf/2104.08718) - for evaluating text-to-image alignment\n",
    "- [Fréchet inception distance (FID)](https://en.wikipedia.org/wiki/Fr%C3%A9chet_inception_distance) - for evaluating image quality\n",
    "- MSE against paper results - for evaluating output stability and consistency\n",
    "\n",
    "Inference time is already collected during generation.\n",
    "\n",
    "CLIP score is a bit tricky to compute because embedding all images at once causes CUDA memory errors. Our approach is to only process one seed at a time. In our `clip_score.py`, we accept one single seed per run, and use Bash to run the script over all seeds. The script will output a `output/{seed}/similarities.csv` file with all CLIP scores for each seed. You can compute one seed with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clip_score import compute_clip_score\n",
    "\n",
    "compute_clip_score(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or compute all of them with the following Bash script:\n",
    "\n",
    "```bash\n",
    "for dir in output/*; do\n",
    "  if [ -d \"$dir\" ]; then\n",
    "    suffix=${dir#output/}\n",
    "    python clip_score.py --seed \"$suffix\"\n",
    "  fi\n",
    "done\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FID scores are directly calculated using the command line. The `pytorch-fid` command line tool accepts one folder of real images and one folder of generated images, so to run it over all groups and output a CSV, we again use Bash:\n",
    "\n",
    "```bash\n",
    "echo \"scheduler,steps,FID\" > fid.csv\n",
    "for dir in output-alt/*; do\n",
    "  fid_output=$(pytorch-fid real_images/ \"$dir\" --batch-size 64 --device cuda | grep 'FID: ' | sed 's/FID: //')\n",
    "  scheduler=$(echo \"$dir\" | cut -d'_' -f1)\n",
    "  steps=$(echo \"$dir\" | cut -d'_' -f2)\n",
    "  echo \"$scheduler,$steps,$fid_output\" >> fid.csv\n",
    "done\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "We first create collages for some sample images. This is implemented in `plot_output.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_output import make_demos\n",
    "\n",
    "make_demos(431, \"plots/output_demos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot for the scheduled timesteps (`plot_schedule.py`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_schedule import plot_schedule\n",
    "\n",
    "plot_schedule(\"plots/schedule.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot for inference time (`plot_times.py`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_times import plot_times\n",
    "\n",
    "plot_times(\"plots/time.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot for CLIP scores (`plot_similarities.py`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_similarities import plot_similarities\n",
    "\n",
    "plot_similarities(\"plots/similarity.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot for FID scores (`plot_fid.py`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_fid import plot_fid\n",
    "\n",
    "plot_fid(\"plots/fid_score.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot for MSE against paper results (`mse_score.py`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mse_score import plot_mse\n",
    "\n",
    "plot_mse(\"plots/mse_score.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
